{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitvoicecheckdeepfakeconda9bcd5623545c494b9292d4fa89ec586c",
   "display_name": "Python 3.7.6 64-bit ('voicecheckdeepfake': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import functools\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "the device being used is:  cuda\n"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('the device being used is: ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_path = './state_lr-0.003_2020-03-24-14-08-18.tar'\n",
    "test_audio_dir = '/home/laurence/Documents/machine-learning/deepfake/soundwaves/data/audio_dfdc_train_part_49/'\n",
    "output_dir = './output/'\n",
    "input_dir = './input/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windowed_tensor(input, window_size):\n",
    "    input = input.view([1,input.shape[0],input.shape[1]])\n",
    "    windows_num = math.ceil(input.shape[2]/window_size)\n",
    "    stacked_partials = []\n",
    "    for audio in input:\n",
    "        audio_windows = []\n",
    "        for window_n in range(windows_num):\n",
    "            window_tensor = audio[:,window_n*window_size:(window_n+1)*window_size]\n",
    "            if window_tensor.shape[1] < window_size:\n",
    "                temp = torch.zeros(audio.shape[0], window_size)\n",
    "                temp[:,:window_tensor.shape[1]] = window_tensor\n",
    "                window_tensor = temp\n",
    "\n",
    "            audio_windows.append(window_tensor)\n",
    "        stacked_partials.append(torch.stack(audio_windows))\n",
    "    stacked_total = torch.cat(stacked_partials)\n",
    "    return stacked_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_input(video):\n",
    "    audio_path = video\n",
    "    waveform, sr = torchaudio.load(audio_path, out = None, normalization = True, channels_first=False)\n",
    "    waveform = waveform.permute(1,0)\n",
    "    windowed_waveform = create_windowed_tensor(waveform, 32000)\n",
    "    return windowed_waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Net(\n  (conv1): Conv1d(1, 16, kernel_size=(64,), stride=(2,))\n  (bn1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (pool1): MaxPool1d(kernel_size=8, stride=8, padding=0, dilation=1, ceil_mode=False)\n  (conv2): Conv1d(16, 32, kernel_size=(32,), stride=(2,))\n  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (pool2): MaxPool1d(kernel_size=8, stride=8, padding=0, dilation=1, ceil_mode=False)\n  (conv3): Conv1d(32, 64, kernel_size=(16,), stride=(2,))\n  (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (conv4): Conv1d(64, 128, kernel_size=(8,), stride=(2,))\n  (bn4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (conv5): Conv1d(128, 256, kernel_size=(4,), stride=(2,))\n  (bn5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (pool3): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n  (dropout): Dropout(p=0.25, inplace=False)\n  (fc1): Linear(in_features=512, out_features=128, bias=True)\n  (fc2): Linear(in_features=128, out_features=64, bias=True)\n  (fc3): Linear(in_features=64, out_features=2, bias=True)\n)\n"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, 64, stride=2)\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.pool1 = nn.MaxPool1d(8, stride=8)\n",
    "        self.conv2 = nn.Conv1d(16, 32, 32, stride=2)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.pool2 = nn.MaxPool1d(8, stride=8)\n",
    "        self.conv3 = nn.Conv1d(32, 64, 16, stride=2)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.conv4 = nn.Conv1d(64, 128, 8, stride=2)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.conv5 = nn.Conv1d(128, 256, 4, stride=2)\n",
    "        self.bn5 = nn.BatchNorm1d(256)\n",
    "        self.pool3 = nn.MaxPool1d(4, stride=4)\n",
    "        self.dropout = nn.Dropout(p=0.25)\n",
    "        self.input_linear = 256*2\n",
    "        self.fc1 = nn.Linear(self.input_linear, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.bn3(x))\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.bn4(x))\n",
    "        x = self.conv5(x)\n",
    "        x = F.relu(self.bn5(x))\n",
    "        x = self.pool3(x)\n",
    "        x = x.view(-1, self.input_linear)\n",
    "        x = self.dropout(self.fc1(x)) #apply dropout on the fc layer\n",
    "        x = self.dropout(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "model = Net()\n",
    "params = model.parameters()\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Net(\n  (conv1): Conv1d(1, 16, kernel_size=(64,), stride=(2,))\n  (bn1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (pool1): MaxPool1d(kernel_size=8, stride=8, padding=0, dilation=1, ceil_mode=False)\n  (conv2): Conv1d(16, 32, kernel_size=(32,), stride=(2,))\n  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (pool2): MaxPool1d(kernel_size=8, stride=8, padding=0, dilation=1, ceil_mode=False)\n  (conv3): Conv1d(32, 64, kernel_size=(16,), stride=(2,))\n  (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (conv4): Conv1d(64, 128, kernel_size=(8,), stride=(2,))\n  (bn4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (conv5): Conv1d(128, 256, kernel_size=(4,), stride=(2,))\n  (bn5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (pool3): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n  (dropout): Dropout(p=0.25, inplace=False)\n  (fc1): Linear(in_features=512, out_features=128, bias=True)\n  (fc2): Linear(in_features=128, out_features=64, bias=True)\n  (fc3): Linear(in_features=64, out_features=2, bias=True)\n)"
     },
     "metadata": {},
     "execution_count": 107
    }
   ],
   "source": [
    "checkpoint = torch.load(saved_model_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# labels = pd.read_pickle('/home/laurence/Documents/machine-learning/deepfake/soundwaves/data/dfdc_train_part_49/output/uniques_pickle.pkl')\n",
    "# correct = 0\n",
    "# correct_finals = [0,0,0]\n",
    "# count_windows = 0\n",
    "# count = 0\n",
    "# for filename in os.listdir(test_audio_dir):\n",
    "#     count += 1\n",
    "#     no_ext = filename[:-4]\n",
    "#     for label in labels:\n",
    "#         if label[0] == no_ext:\n",
    "#             labelino = label[1]\n",
    "#             # print(f'the label for {no_ext} is {label[1]}')\n",
    "#     data = feed_input(test_audio_dir + filename)\n",
    "#     data = data.to(device)\n",
    "#     output = model(data)\n",
    "#     window_predictions = []\n",
    "#     for window in output:\n",
    "#         count_windows += 1\n",
    "#         prediction = torch.argmax(window)\n",
    "#         window_predictions.append(prediction)\n",
    "#         if labelino == prediction.item():\n",
    "#             correct += 1\n",
    "#     fake_window_num = window_predictions.count(1)\n",
    "#     final_prediction = [0,0,0]\n",
    "#     if fake_window_num == 1:\n",
    "#         final_prediction[0] += 1\n",
    "#     elif fake_window_num == 2:\n",
    "#         final_prediction[0] += 1\n",
    "#         final_prediction[1] += 1\n",
    "#     elif fake_window_num > 2:\n",
    "#         final_prediction[0] += 1\n",
    "#         final_prediction[1] += 1\n",
    "#         final_prediction[2] += 1\n",
    "\n",
    "#     for final_pred_idx in range(0, 3):\n",
    "#         if final_prediction[final_pred_idx] == labelino:\n",
    "#             for correct_idx in range (0, final_pred_idx+1):\n",
    "#                 correct_finals[correct_idx] += 1\n",
    "\n",
    "    \n",
    "# print(f'there have been {correct} correct window classifications over the whole {count_windows} range')\n",
    "# for cf in correct_finals:\n",
    "#     print(f'there have been {cf} correct classifications over the whole {count} range')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio(video_name):\n",
    "    video_path =  os.path.join(input_dir, video_name)\n",
    "    output_wav_path = os.path.join(output_dir, video_name[:-4] + \".wav\")\n",
    "    probe_real = f\"ffprobe -show_streams -print_format json {video_path} | grep -o 'Audio'\"\n",
    "    cmd_real = subprocess.run(probe_real,shell=True,stdout=subprocess.PIPE,stderr=subprocess.STDOUT)\n",
    "\n",
    "    if \"Audio\" in cmd_real.stdout.decode('utf-8'):\n",
    "        if not os.path.exists(output_wav_path):\n",
    "            print(\"extracting audio from video\")\n",
    "            subprocess.call(['ffmpeg', '-i', video_path, '-max_muxing_queue_size', '9999',output_wav_path])\n",
    "    return output_wav_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windows_prediction_aggregator(window_predictions):\n",
    "    final_prediction = 0\n",
    "    for window_pred in window_predictions:\n",
    "        if window_pred == 1:\n",
    "            final_prediction = 1\n",
    "    return final_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(video_name):\n",
    "    # create the audio file and assign the path to a var\n",
    "    audio_path = extract_audio(video_name)\n",
    "    # feed the audio file as a tensor and split it in windows\n",
    "    data = feed_input(audio_path)\n",
    "    data = data.to(device)\n",
    "    # determine the predictions over each window\n",
    "    window_output = model(data)\n",
    "    # aggregate the predictions on the windows: if any window is predicted to be fake the whole video is considered fake\n",
    "    prediction = windows_prediction_aggregator(window_output)\n",
    "    print(f'The video {video_name} is predicted to be {\"real\" if prediction == 0 else \"fake\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'./output/./input/1.wav'"
     },
     "metadata": {},
     "execution_count": 118
    }
   ],
   "source": [
    "predict('1.mp4')\n",
    "# extract_audio('./input/1.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "extracting audio from video\n"
    }
   ],
   "source": []
  }
 ]
}