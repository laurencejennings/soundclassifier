{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitvoicecheckdeepfakeconda9bcd5623545c494b9292d4fa89ec586c",
   "display_name": "Python 3.7.6 64-bit ('voicecheckdeepfake': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    " \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import functools\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset\n",
    "import torchaudio\n",
    "from random import seed\n",
    "from random import randint\n",
    "from sklearn.utils import shuffle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/media/laurence/black/deepfake_challenge/data\"\n",
    "# base_dir = \"/home/laurence/Documents/machine-learning/deepfake/soundwaves/data/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunk_labels(n_chunk):\n",
    "# numbers of dataset chunks to use:\n",
    "    chunk_num = n_chunk\n",
    "    data_chunk = f'dfdc_train_part_{chunk_num}'\n",
    "    output_dir = os.path.join(base_dir, data_chunk, \"output\")\n",
    "    audio_labels = os.path.join(output_dir, \"uniques_pickle.pkl\")\n",
    "    unpickled_df = pd.read_pickle(audio_labels)\n",
    "    return unpickled_df\n",
    "\n",
    "\n",
    "# randomize the pickle and split the data:\n",
    "def randomize_pickle(pickle, seed):\n",
    "    np_unpickled_df = np.asarray(pickle)\n",
    "    shuffled_df = shuffle(np_unpickled_df, random_state=seed)\n",
    "    train, test = np.split(shuffled_df, [int(.8*len(shuffled_df))])\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def append_to_dict(elements_dict, pair, chunk):\n",
    "    elements_dict.update({pair[0].strip():{'chunk':chunk, 'label': 0}}) # 0 = fake\n",
    "    elements_dict.update({pair[1].strip():{'chunk':chunk, 'label': 1}}) # 1 = real\n",
    "\n",
    "def append_to_dict_v2(elements_dict, name, label, chunk):\n",
    "    elements_dict.update({name.strip():{'chunk':chunk, 'label': label}}) \n",
    "\n",
    "\n",
    "def get_chunks(chunk_list):\n",
    "    seed = randint(1, 1000)\n",
    "    train_lables = {}\n",
    "    test_lables = {}\n",
    "    for chunk in chunk_list:\n",
    "        pickle = get_chunk_labels(chunk)\n",
    "        print(f'adding chunk #{chunk} of length: {len(pickle)}')\n",
    "        randomized_train, randomized_test = randomize_pickle(pickle, seed)\n",
    "        for pair in randomized_train:\n",
    "            append_to_dict(train_lables, pair, chunk)\n",
    "        for pair in randomized_test:\n",
    "            append_to_dict(test_lables, pair, chunk)\n",
    "    return train_lables, test_lables\n",
    "\n",
    "def get_chunks_v2(chunk_list):\n",
    "    seed = randint(1, 1000)\n",
    "    train_lables = {}\n",
    "    test_lables = {}\n",
    "    for chunk in chunk_list:\n",
    "        pickle = get_chunk_labels(chunk)\n",
    "        print(f'adding chunk #{chunk} of length: {len(pickle)}')\n",
    "        randomized_train, randomized_test = randomize_pickle(pickle, seed)\n",
    "        for video in randomized_train:\n",
    "            append_to_dict_v2(train_lables, video[0], int(video[1]), chunk)\n",
    "        for pair in randomized_test:\n",
    "            append_to_dict_v2(test_lables, video[0], int(video[1]), chunk)\n",
    "    return train_lables, test_lables\n",
    "    \n",
    "def get_audio_chunk_dir(chunk_num):\n",
    "    data_chunk = f'audio_dfdc_train_part_{chunk_num}'\n",
    "    chunk_dir = os.path.join(base_dir, data_chunk)\n",
    "    return chunk_dir\n",
    "\n",
    "def get_torchaudio_file(filename):\n",
    "    audio_file = os.path.join(get_chunk_dir(randomized_train[filename].get('chunk')), filename + \".wav\")\n",
    "    waveform, sample_rate = torchaudio.load(audio_file)\n",
    "    return audio_file, waveform, sample_rate\n",
    "\n",
    "def get_chunk_dir(chunk_num):\n",
    "    data_chunk = f'audio_dfdc_train_part_{chunk_num}'\n",
    "    chunk_dir = os.path.join(base_dir, data_chunk)\n",
    "    return chunk_dir\n",
    "\n",
    "def get_file(dataset, filename):\n",
    "    audio_file = os.path.join(get_chunk_dir(dataset[filename].get('chunk')), filename + \".wav\")\n",
    "    waveform, sample_rate = torchaudio.load(audio_file)\n",
    "    return audio_file, waveform, sample_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "the device being used is:  cuda\n"
    }
   ],
   "source": [
    "\n",
    "# Letâ€™s check if a CUDA GPU is available and select our device. Running\n",
    "# the network on a GPU will greatly decrease the training/testing runtime.\n",
    "# \n",
    "# \n",
    "# \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('the device being used is: ', device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "adding chunk #10 of length: 3192\nadding chunk #12 of length: 2225\n"
    }
   ],
   "source": [
    "# Creating the data variable\n",
    "# --------------------------\n",
    "# the dataset variables randomized_train and randomized_test contain pre randomized split datasets from the total number of files used in the dataset. The function get_chunks gets a list of numbers which are the zip file numbers found in the original datasets of dfdc. The function get_chunks outputs the number of audio pairs in the chunk. \n",
    "# \n",
    "# ### TODO, should use also the videos with real audio files to train on more data. \n",
    "\n",
    "# randomized_train, randomized_test = get_chunks([0, 2, 23, 40, 41, 42, 43, 45, 46, 47, 48, 49])\n",
    "randomized_train, randomized_test = get_chunks_v2([10])\n",
    "randomized_train, randomized_test = get_chunks_v2([12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "hibkmwmyum {'chunk': 12, 'label': 0}\n"
    }
   ],
   "source": [
    "for video in randomized_train:\n",
    "    print(video, randomized_train[video])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "present sample rates in the dataset:\n[48000, 44100]\nsample rates occurrence:\n{48000: 1703, 44100: 77}\nmax sample rate found:\n48000\npresent sample numbers in the dataset:\n[480256, 441344]\nsample numbers occurrence:\n{480256: 1703, 441344: 77} \n\ntime durations present in the dataset:\n[10.005333333333333, 10.00780045351474]\ntime durations occurrence:\n{10.005333333333333: 1703, 10.00780045351474: 77} \n\npresent channels in the dataset:\n[1]\nchannels occurrence:\n{1: 1780} \n\n"
    }
   ],
   "source": [
    "channels = {}\n",
    "known_chans = []\n",
    "samples_l = {}\n",
    "known_sl = []\n",
    "durations_n = {}\n",
    "known_dn = []\n",
    "sample_rates = {}\n",
    "known_sr = []\n",
    "for filename in randomized_train:\n",
    "    audio_file, waveform, sample_rate = get_file(randomized_train, filename)\n",
    "    \n",
    "    if sample_rate in known_sr:\n",
    "        sample_rates[sample_rate] = sample_rates[sample_rate] + 1\n",
    "    else:\n",
    "        known_sr.append(sample_rate)\n",
    "        sample_rates[sample_rate] = 1\n",
    "\n",
    "    if waveform.shape[0] in known_chans:\n",
    "        channels[waveform.shape[0]] = channels[waveform.shape[0]] + 1\n",
    "    else:\n",
    "        known_chans.append(waveform.shape[0])\n",
    "        channels[waveform.shape[0]] = 1\n",
    "\n",
    "    if waveform.shape[1] in known_sl:\n",
    "        samples_l[waveform.shape[1]] = samples_l[waveform.shape[1]] + 1\n",
    "    else:\n",
    "        known_sl.append(waveform.shape[1])\n",
    "        samples_l[waveform.shape[1]] = 1\n",
    "\n",
    "    if waveform.shape[1]/sample_rate in known_dn:\n",
    "        durations_n[waveform.shape[1]/sample_rate] = durations_n[waveform.shape[1]/sample_rate] + 1\n",
    "    else:\n",
    "        known_dn.append(waveform.shape[1]/sample_rate)\n",
    "        durations_n[waveform.shape[1]/sample_rate] = 1\n",
    "\n",
    "print(\"present sample rates in the dataset:\")\n",
    "print(known_sr)\n",
    "print(\"sample rates occurrence:\")\n",
    "print(sample_rates)\n",
    "print(\"max sample rate found:\")\n",
    "print(max(known_sr))\n",
    "\n",
    "print(\"present sample numbers in the dataset:\")\n",
    "print(known_sl)\n",
    "print(\"sample numbers occurrence:\")\n",
    "print(samples_l, '\\n')\n",
    "\n",
    "print('time durations present in the dataset:')\n",
    "print(known_dn)\n",
    "print(\"time durations occurrence:\")\n",
    "print(durations_n, '\\n')\n",
    "\n",
    "print('present channels in the dataset:')\n",
    "print(known_chans)\n",
    "print('channels occurrence:')\n",
    "print(channels, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windowed_tensor(input, window_size, labels):\n",
    "    # input: the input tensor \n",
    "    windows_num = math.ceil(input.shape[2]/window_size)\n",
    "    stacked_partials = []\n",
    "    stretched_labels = []\n",
    "    label_index = 0\n",
    "    for audio in input:\n",
    "        audio_windows = []\n",
    "        for window_n in range(windows_num):\n",
    "            window_tensor = audio[:,window_n*window_size:(window_n+1)*window_size]\n",
    "            if window_tensor.shape[1] < window_size:\n",
    "                temp = torch.zeros(audio.shape[0], window_size)\n",
    "                temp[:,:window_tensor.shape[1]] = window_tensor\n",
    "                window_tensor = temp\n",
    "\n",
    "            audio_windows.append(window_tensor)\n",
    "        stacked_partials.append(torch.stack(audio_windows))\n",
    "        for i in range(len(audio_windows)):\n",
    "            stretched_labels.append(labels[label_index])\n",
    "        label_index += 1\n",
    "    stacked_total = torch.cat(stacked_partials)\n",
    "    stretched_labels = torch.stack(stretched_labels)\n",
    "    # print(stacked_total.shape, stretched_labels.shape)\n",
    "    return stacked_total, stretched_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Train set size: 1780\nTest set size: 1\n"
    }
   ],
   "source": [
    "\n",
    "class DeepfakeVoiceDataset(Dataset):\n",
    "#rapper for the DeepfakeVoice dataset\n",
    "    # Argument List\n",
    "    #  dictionary dataset\n",
    "    #  window size int\n",
    "    #  window_overlap float\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        self.file_names = []\n",
    "        self.labels = []\n",
    "        self.folders = []\n",
    "        for video in dataset:\n",
    "            self.file_names.append(video + \".wav\")\n",
    "            self.labels.append(dataset[video].get('label'))\n",
    "            self.folders.append(dataset[video].get('chunk'))\n",
    "                \n",
    "        self.mixer = torch.mean #UrbanSound8K uses two channels, this will convert them to one\n",
    "    def __getitem__(self, index):\n",
    "        #format the file path and load the file\n",
    "        path = os.path.join(get_chunk_dir(self.folders[index]), self.file_names[index])\n",
    "\n",
    "        # normalization True is equal to normalization = 32 and it scales each datapoint from a 2**32\n",
    "        # order of magnitude number to a 2**0 order of magnitude one. This can be changed to\n",
    "        # normalization = 16 or normalization = Function\n",
    "        waveform, sr = torchaudio.load(path, out = None, normalization = True, channels_first=False)\n",
    "        waveform = waveform.permute(1,0)\n",
    "        # make all inputs the same size based on the longest sample length \n",
    "        tempData = torch.zeros([1, max(known_sl)]) \n",
    "        tempData[:,:waveform.numel()] = waveform[:]\n",
    "\n",
    "        # if waveform.numel() < max(known_sl):\n",
    "        #     tempData[:,:waveform.numel()] = waveform[:]\n",
    "        # else:\n",
    "        #     tempData[:] = waveform[:max(known_sl)]\n",
    "        \n",
    "        waveform = tempData\n",
    "        return waveform, self.labels[index]\n",
    " \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "     \n",
    "    \n",
    "train_set = DeepfakeVoiceDataset(randomized_train) \n",
    "test_set = DeepfakeVoiceDataset(randomized_test)\n",
    "print(\"Train set size: \" + str(len(train_set)))\n",
    "print(\"Test set size: \" + str(len(test_set)))\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if device == 'cuda' else {} #needed for using datasets on gpu\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size = 4, shuffle = True, **kwargs) #changed from 64\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size = 4, shuffle = True, **kwargs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Net(\n  (conv1): Conv1d(1, 16, kernel_size=(64,), stride=(2,))\n  (bn1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (pool1): MaxPool1d(kernel_size=8, stride=8, padding=0, dilation=1, ceil_mode=False)\n  (conv2): Conv1d(16, 32, kernel_size=(32,), stride=(2,))\n  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (pool2): MaxPool1d(kernel_size=8, stride=8, padding=0, dilation=1, ceil_mode=False)\n  (conv3): Conv1d(32, 64, kernel_size=(16,), stride=(2,))\n  (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (conv4): Conv1d(64, 128, kernel_size=(8,), stride=(2,))\n  (bn4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (conv5): Conv1d(128, 256, kernel_size=(4,), stride=(2,))\n  (bn5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (pool3): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n  (dropout): Dropout(p=0.25, inplace=False)\n  (fc1): Linear(in_features=512, out_features=128, bias=True)\n  (fc2): Linear(in_features=128, out_features=64, bias=True)\n  (fc3): Linear(in_features=64, out_features=2, bias=True)\n)\n"
    }
   ],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, 64, stride=2)\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.pool1 = nn.MaxPool1d(8, stride=8)\n",
    "        self.conv2 = nn.Conv1d(16, 32, 32, stride=2)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.pool2 = nn.MaxPool1d(8, stride=8)\n",
    "        self.conv3 = nn.Conv1d(32, 64, 16, stride=2)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.conv4 = nn.Conv1d(64, 128, 8, stride=2)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.conv5 = nn.Conv1d(128, 256, 4, stride=2)\n",
    "        self.bn5 = nn.BatchNorm1d(256)\n",
    "        self.pool3 = nn.MaxPool1d(4, stride=4)\n",
    "        self.dropout = nn.Dropout(p=0.25)\n",
    "        self.input_linear = 256*2\n",
    "        self.fc1 = nn.Linear(self.input_linear, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        x = self.conv1(x)\n",
    "        # print(x.shape)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        # print(x.shape)\n",
    "        x = self.pool1(x)\n",
    "        # print(x.shape)\n",
    "        x = self.conv2(x)\n",
    "        # print(x.shape)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        # print(x.shape)\n",
    "        x = self.pool2(x)\n",
    "        # print(x.shape)\n",
    "        x = self.conv3(x)\n",
    "        # print(x.shape)\n",
    "        x = F.relu(self.bn3(x))\n",
    "        # print(x.shape)\n",
    "        x = self.conv4(x)\n",
    "        # print(x.shape)\n",
    "        x = F.relu(self.bn4(x))\n",
    "        # print(x.shape)\n",
    "        x = self.conv5(x)\n",
    "        # print(x.shape)\n",
    "        x = F.relu(self.bn5(x))\n",
    "        # print(x.shape)\n",
    "        x = self.pool3(x)\n",
    "        # print(x.shape)\n",
    "        x = x.view(-1, self.input_linear)\n",
    "        x = self.dropout(self.fc1(x)) #apply dropout on the fc layer\n",
    "        x = self.dropout(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "model = Net()\n",
    "params = model.parameters()\n",
    "model.to(device)\n",
    "print(model)\n",
    "lr=0.003\n",
    "#The Adadelta ( Zeiler, 2012 ) optimizer with the default learning rate of 1.0 was used. Adadelta has been chosen because this method dynamically adapts the learning rate during the optimization process.\n",
    "optimizer = torch.optim.Adadelta(params, lr=lr, rho=0.9, eps=1e-06, weight_decay=0) \n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 20, gamma = 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-23-a93059cea6d9>, line 81)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-23-a93059cea6d9>\"\u001b[0;36m, line \u001b[0;32m81\u001b[0m\n\u001b[0;31m    }, f'./state_lr-{lr}_{datetime.today().strftime('%Y-%m-%d-%H-%M-%S')}.tar')\u001b[0m\n\u001b[0m                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# define parameters of training:\n",
    "epochs = 1\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "print_every = 30\n",
    "train_losses, test_losses = [], []\n",
    "trained_on = 0\n",
    "\n",
    "# loop over epoch range\n",
    "for epoch in range(epochs):\n",
    "    # perform scheduler step to reduce learning rate\n",
    "    scheduler.step()\n",
    "    print('Epoch {}, lr {}'.format(epoch, optimizer.param_groups[0]['lr']))\n",
    "    trained_on = 0\n",
    "\n",
    "    # loop over all inputs in train loader\n",
    "    for inputs, labels in train_loader:\n",
    "        # print inputs size from the train loader\n",
    "        # print(f'inputs from train_loader: {inputs.shape}')\n",
    "        # convert train loader inputs into window of inputs\n",
    "        inputs, labels = create_windowed_tensor(inputs, 32000, labels)\n",
    "        # print inputs size from the windows\n",
    "        # print(f'inputs from windows: {inputs.shape}')\n",
    "        # adjust trained on to consider all windows for data augmentation\n",
    "        trained_on += inputs.shape[0]\n",
    "        # using the longest sample length, divided by the window size, determine the num of windows\n",
    "        total_windows = math.ceil(max(known_sl)/32000)\n",
    "        # calculate percentage of training samples used\n",
    "        trained_pct = math.floor(trained_on*100/(len(train_set)*total_windows))\n",
    "        \n",
    "        # increase step counter used in printing calculation\n",
    "        steps += 1\n",
    "        # pass inputs and labels (windowed) to device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # zero the gradients before backprop\n",
    "        optimizer.zero_grad()\n",
    "        # calculate predictions using model and windowed inputs\n",
    "        predictions = model.forward(inputs)\n",
    "        # calculate loss using predictions\n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # add the loss of this batch to the running loss\n",
    "        running_loss += loss.item()\n",
    "        # print(f'running loss: {running_loss}')\n",
    "        \n",
    "        # if the step is one in which printing is necessary load the test data and run a test round.\n",
    "        if steps % print_every == 0:\n",
    "            test_loss = 0\n",
    "            accuracy = 0\n",
    "            # switch the model to eval mode: batchnorm or dropout layers will work in eval mode instead of training mode\n",
    "            model.eval()\n",
    "            # use no grad as the test items shouldn't have impact on the training and gradient\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in test_loader:\n",
    "                    inputs, labels = create_windowed_tensor(inputs, 32000, labels)\n",
    "                    inputs, labels = inputs.to(device),labels.to(device)\n",
    "                    predictions = model.forward(inputs)\n",
    "                    batch_loss = criterion(predictions, labels)\n",
    "                    test_loss += batch_loss.item()\n",
    "                    \n",
    "                    ps = torch.exp(predictions)\n",
    "                    top_p, top_class = ps.topk(1, dim=1)\n",
    "                    equals = top_class == labels.view(*top_class.shape)\n",
    "                    accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "            train_losses.append(running_loss/print_every)\n",
    "            test_losses.append(test_loss/len(test_loader))  \n",
    "            print(f\"Epoch {epoch+1}/{epochs}.. tested {trained_pct}%/100% \"\n",
    "                  f\"Train loss: {running_loss/print_every:.3f}.. \"\n",
    "                  f\"Test loss: {test_loss/len(test_loader):.3f}.. \"\n",
    "                  f\"Test accuracy: {accuracy/len(test_loader):.3f}\")\n",
    "            running_loss = 0\n",
    "            model.train()\n",
    "    print(f'train losses: {train_losses}')\n",
    "    print(f'epoch: {epoch} finished. Saving')\n",
    "    torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, f'./state_lr-{lr}_{datetime.today().strftime('%Y-%m-%d-%H-%M-%S')}.tar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}